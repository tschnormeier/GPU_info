{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyncy7iTIev/rq8wK7cvNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tschnormeier/GPU_info/blob/main/GPU_info_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "62A6i5SMygP-"
      },
      "outputs": [],
      "source": [
        "# some code from stackoverflow on detecting a GPU\n",
        "# 'how do I check if pytorch is using the gpu'\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpC_SvWGylHx",
        "outputId": "df146e0d-3ea7-4cb4-bd7b-d5d6e08d1bac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Info when using cuda\n",
        "if device.type == 'cuda':\n",
        "    # Ops the next two lines were for TensorFlow\n",
        "    #numGPUs = len(tf.config.list_physical_devices('GPU'))\n",
        "    #print('Number of GPUs Available: ', numGPUs)\n",
        "    \n",
        "    # Get the number of GPUs available using PyTorch\n",
        "    numGPUs = torch.cuda.device_count()\n",
        "    print('Number of GPUs Available: ', numGPUs)\n",
        "\n",
        "\n",
        "    # Loop through GPUs and print information\n",
        "    for i in range(0, numGPUs):\n",
        "        print('\\n***  GPU number {} information ***'.format(i+1))\n",
        "        print(torch.cuda.get_device_name(0))\n",
        "        print('Memory Usage:')\n",
        "        print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1))\n",
        "        print('Cached:   ', round(torch.cuda.memory_reserved(0) /1024**3,1))\n",
        "        global_free, total_GPU_memory = torch.cuda.mem_get_info()\n",
        "        print('global free memory: {} bytes, total GPU memory: {} bytes'.format(global_free, total_GPU_memory))\n",
        "        print('global free memory: {} GB, total GPU memory: {} GB'.format(\n",
        "            round(global_free / 1024**3,3), round(total_GPU_memory / 1024**3,3)))\n",
        "        a = torch.cuda.get_device_properties(0)\n",
        "        print('Cuda Device Properties:')\n",
        "        print('    name:  {}'.format(a.name))\n",
        "        print('    major: {}, minor: {}'.format(a.major, a.minor))\n",
        "        #print('    minor: {}'.format(a.minor))\n",
        "        print('    total_memory: {}'.format(a.total_memory))\n",
        "        print('    multi_processor_count: {}\\n'.format(a.multi_processor_count))\n",
        "else:\n",
        "  print('Using CPU not GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVs5a4HayuXO",
        "outputId": "7a393e08-8839-44fe-fcae-88c4ee4f3866"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs Available:  1\n",
            "\n",
            "***  GPU number 1 information ***\n",
            "Tesla T4\n",
            "Memory Usage:\n",
            "Allocated: 0.0\n",
            "Cached:    0.0\n",
            "global free memory: 15727394816 bytes, total GPU memory: 15835398144 bytes\n",
            "global free memory: 14.647 GB, total GPU memory: 14.748 GB\n",
            "Cuda Device Properties:\n",
            "    name:  Tesla T4\n",
            "    major: 7, minor: 5\n",
            "    total_memory: 15835398144\n",
            "    multi_processor_count: 40\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HnIid1MRy9_g"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}